{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "mount_file_id": "1NX-llHqYAfCraylQ7aoi4z_mZOk3LsFw",
   "authorship_tag": "ABX9TyOpruOa1loMpPR/BKsp6A3r"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "premium"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install Pillow\n",
    "!pip install psutil"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from typing import Callable, Optional\n",
    "from PIL import Image\n",
    "\n",
    "from psutil import virtual_memory\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "#@title Show Runtime Information\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)\n",
    "\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "    print('Not using a high-RAM runtime')\n",
    "else:\n",
    "    print('You are using a high-RAM runtime!')\n",
    "\n",
    "\n",
    "class FilteredDataLoader(DataLoader):\n",
    "    def __iter__(self):\n",
    "        for batch in super().__iter__():\n",
    "            inputs, targets = batch\n",
    "            filtered_indices = [i for i, img in enumerate(inputs) if img is not None]\n",
    "            filtered_inputs = inputs[filtered_indices]\n",
    "            filtered_targets = targets[filtered_indices]\n",
    "            yield filtered_inputs, filtered_targets\n",
    "\n",
    "\n",
    "class CustomImageFolder(datasets.ImageFolder):\n",
    "    def __init__(self, root: str, transform: Optional[Callable] = None):\n",
    "        def is_valid_file(path: str) -> bool:\n",
    "            try:\n",
    "                with open(path, \"rb\") as f:\n",
    "                    Image.open(f).verify()\n",
    "                return True\n",
    "            except (OSError, IOError):\n",
    "                print(f\"Warning: Could not read image {path}, skipping.\")\n",
    "                return False\n",
    "\n",
    "        super().__init__(root, transform=transform, is_valid_file=is_valid_file)\n",
    "\n",
    "    def _load_image(self, path: str) -> Optional[Image.Image]:\n",
    "        try:\n",
    "            return Image.open(path).convert(\"RGB\")\n",
    "        except OSError:\n",
    "            print(f\"Warning: Could not read image {path}, skipping.\")\n",
    "            return None\n",
    "\n",
    "    def _filter_corrupted_images(self, root: str):\n",
    "        samples = []\n",
    "        targets = []\n",
    "        for i, (path, target) in enumerate(datasets.ImageFolder(root).samples):\n",
    "            img = self._load_image(path)\n",
    "            if img is not None:\n",
    "                samples.append((path, target))\n",
    "                targets.append(target)\n",
    "        return samples, targets\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        path, target = self.samples[index]\n",
    "        img = self._load_image(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "print('Loading and preprocessing the dataset...')\n",
    "model_path = '/your/path/datasets/gpt-4-cnn-starter.pth'\n",
    "image_path = '/your/path/image.png'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_data = CustomImageFolder(root='/your/path/train', transform=transform)\n",
    "test_data = CustomImageFolder(root='/your/path/test', transform=transform)\n",
    "\n",
    "train_loader = FilteredDataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = FilteredDataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the CNN architecture\n",
    "print('Defining the CNN architecture...')\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 56 * 56, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "num_classes = len(train_data.classes)\n",
    "model = SimpleCNN(num_classes)\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "print('Setting up loss function and optimizer...')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "print('Training the model...')\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs} Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "print('Evaluating the model...')\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        print(f'Evaluating image batch {i+1}/{len(test_loader)}')\n",
    "        for j in range(inputs.size(0)):\n",
    "            filename = test_data.samples[i*inputs.size(0) + j][0]\n",
    "            outputs = model(inputs[j:j+1])\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += 1\n",
    "            correct += predicted.eq(targets[j:j+1]).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test accuracy: {accuracy:.2%}')\n",
    "\n",
    "# Save the trained model's state dictionary\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Create a model instance and load the state dictionary\n",
    "model = SimpleCNN(num_classes)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Print the model's architecture\n",
    "print(model)\n",
    "\n",
    "# Print individual layer weights\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.size()}\")\n",
    "\n",
    "# Create a dictionary to map class indices to labels\n",
    "idx_to_class = {idx: label for label, idx in train_data.class_to_idx.items()}\n",
    "\n",
    "# Print the mapping\n",
    "print(idx_to_class)\n",
    "\n",
    "# Example: Get the label for class index 0\n",
    "class_index = 0\n",
    "class_label = idx_to_class[class_index]\n",
    "print(f\"Class index {class_index} corresponds to label '{class_label}'\")\n",
    "\n",
    "\n",
    "# Load an image and preprocess it\n",
    "print('Loading and preprocessing an image...')\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "preprocessed_image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Run the model on the preprocessed image\n",
    "print('Running the model on the image...')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(preprocessed_image)\n",
    "    _, predicted_class_index = outputs.max(1)\n",
    "\n",
    "predicted_label = idx_to_class[predicted_class_index.item()]\n",
    "print(f\"The predicted label for the image is '{predicted_label}'.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
